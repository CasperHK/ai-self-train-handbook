---
sidebar_position: 1
---

# è³‡æ–™æ”¶é›†

é«˜å“è³ªçš„è¨“ç·´è³‡æ–™æ˜¯æˆåŠŸå¾®èª¿ AI æ¨¡å‹çš„é—œéµã€‚æœ¬æŒ‡å—å°‡æ•™ä½ å¦‚ä½•æœ‰æ•ˆæ”¶é›†å’Œçµ„ç¹”è¨“ç·´è³‡æ–™ã€‚

## ğŸ“Š è³‡æ–™éœ€æ±‚è©•ä¼°

### è³‡æ–™é‡å»ºè­°

| ä»»å‹™è¤‡é›œåº¦ | æœ€å°‘è³‡æ–™é‡ | æ¨è–¦è³‡æ–™é‡ | ç†æƒ³è³‡æ–™é‡ |
|-----------|-----------|-----------|-----------|
| ç°¡å–®ï¼ˆFAQï¼‰ | 50 ç­† | 200-500 ç­† | 1000+ ç­† |
| ä¸­ç­‰ï¼ˆå®¢æœå°è©±ï¼‰ | 100 ç­† | 500-1000 ç­† | 5000+ ç­† |
| è¤‡é›œï¼ˆå°ˆæ¥­é ˜åŸŸï¼‰ | 500 ç­† | 2000-5000 ç­† | 10000+ ç­† |

:::tip å“è³ª > æ•¸é‡
10 ç­†é«˜å“è³ªè³‡æ–™çš„æ•ˆæœ > 100 ç­†ä½å“è³ªè³‡æ–™
:::

### è³‡æ–™å¤šæ¨£æ€§

ç¢ºä¿è³‡æ–™æ¶µè“‹ï¼š
- âœ… ä¸åŒçš„å•é¡Œå½¢å¼
- âœ… å„ç¨®å›ç­”é¢¨æ ¼
- âœ… é‚Šç·£æ¡ˆä¾‹
- âœ… å¸¸è¦‹éŒ¯èª¤
- âœ… å°ˆæ¥­è¡“èªå’Œé€šä¿—èªªæ³•

## ğŸ” è³‡æ–™ä¾†æº

### 1. å…§éƒ¨è³‡æ–™

#### å®¢æœè¨˜éŒ„
- æ­·å²å°è©±è¨˜éŒ„
- å¸¸è¦‹å•é¡Œæ¸…å–®
- å®¢æœå·¥å–®ç³»çµ±

**å„ªé»ï¼š**
- çœŸå¯¦å ´æ™¯
- è²¼åˆéœ€æ±‚
- é«˜åº¦ç›¸é—œ

**å–å¾—æ–¹å¼ï¼š**
```python
# ç¯„ä¾‹ï¼šå¾ CSV è®€å–å®¢æœè¨˜éŒ„
import pandas as pd

df = pd.read_csv('customer_service_logs.csv')
conversations = df[['question', 'answer']].to_dict('records')
```

#### ç”¢å“æ–‡ä»¶
- ä½¿ç”¨æ‰‹å†Š
- FAQ æ–‡ä»¶
- æŠ€è¡“è¦æ ¼
- æ“ä½œæŒ‡å—

#### å…§éƒ¨çŸ¥è­˜åº«
- Wiki é é¢
- è¨“ç·´æ–‡ä»¶
- æ¨™æº–ä½œæ¥­æµç¨‹

### 2. å…¬é–‹è³‡æ–™é›†

#### ä¸­æ–‡å°è©±è³‡æ–™é›†

**LCCCï¼ˆLarge-scale Chinese Conversation Collectionï¼‰**
```python
from datasets import load_dataset

dataset = load_dataset("lccc", "base")
```
- è¦æ¨¡ï¼š1200 è¬å°è©±
- ä¾†æºï¼šç¤¾ç¾¤åª’é«”
- é©ç”¨ï¼šé€šç”¨å°è©±

**Chinese-Chitchat-Corpus**
- GitHub: https://github.com/codemayq/chinese_chatbot_corpus
- è¦æ¨¡ï¼š550 è¬å°è©±
- é©ç”¨ï¼šé–’èŠæ©Ÿå™¨äºº

**WebQA**
```python
from datasets import load_dataset

dataset = load_dataset("suolyer/webqa")
```
- è¦æ¨¡ï¼š42k å•ç­”å°
- é©ç”¨ï¼šå•ç­”ç³»çµ±

#### è‹±æ–‡è³‡æ–™é›†ï¼ˆå¯ç¿»è­¯ï¼‰

**Alpaca**
- 52k æŒ‡ä»¤è³‡æ–™
- æ¶µè“‹å¤šç¨®ä»»å‹™
- é©ç”¨ï¼šæŒ‡ä»¤å¾®èª¿

**Dolly**
- 15k äººå·¥æ¨™è¨»è³‡æ–™
- é–‹æ”¾æˆæ¬Š
- é©ç”¨ï¼šé€šç”¨ä»»å‹™

### 3. åˆæˆè³‡æ–™

ä½¿ç”¨å¤§å‹èªè¨€æ¨¡å‹ç”Ÿæˆè¨“ç·´è³‡æ–™ã€‚

#### ä½¿ç”¨ GPT ç”Ÿæˆ

```python
import openai

def generate_qa_pairs(topic, num_pairs=10):
    prompt = f"""è«‹ç”Ÿæˆ {num_pairs} å€‹é—œæ–¼ã€Œ{topic}ã€çš„å•ç­”å°ã€‚
    æ ¼å¼ï¼š
    Q: å•é¡Œ
    A: è©³ç´°å›ç­”
    
    è¦æ±‚ï¼š
    - å•é¡Œè¦æœ‰è®ŠåŒ–æ€§
    - ç­”æ¡ˆè¦å°ˆæ¥­æº–ç¢º
    - æ¶µè“‹ä¸åŒè§’åº¦
    """
    
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    
    return response.choices[0].message.content

# ä½¿ç”¨ç¯„ä¾‹
qa_pairs = generate_qa_pairs("Python ç¨‹å¼è¨­è¨ˆ", 20)
print(qa_pairs)
```

#### ä½¿ç”¨é–‹æºæ¨¡å‹ç”Ÿæˆ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

def generate_synthetic_data(seed_examples, count=100):
    """æ ¹æ“šç¨®å­ç¯„ä¾‹ç”Ÿæˆæ›´å¤šè³‡æ–™"""
    prompt = f"""åŸºæ–¼ä»¥ä¸‹ç¯„ä¾‹ï¼Œç”Ÿæˆé¡ä¼¼çš„å•ç­”å°ï¼š

{seed_examples}

è«‹ç”Ÿæˆ {count} å€‹æ–°çš„å•ç­”å°ã€‚"""
    
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=2000)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### 4. ç¶²è·¯çˆ¬èŸ²

#### åˆæ³•çˆ¬å–å…¬é–‹è³‡è¨Š

```python
import requests
from bs4 import BeautifulSoup

def scrape_faq(url):
    """çˆ¬å– FAQ é é¢"""
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    qa_pairs = []
    # æ ¹æ“šç¶²ç«™çµæ§‹èª¿æ•´é¸æ“‡å™¨
    questions = soup.select('.question')
    answers = soup.select('.answer')
    
    for q, a in zip(questions, answers):
        qa_pairs.append({
            'question': q.text.strip(),
            'answer': a.text.strip()
        })
    
    return qa_pairs

# ä½¿ç”¨ç¯„ä¾‹
faqs = scrape_faq('https://example.com/faq')
```

:::warning æ³¨æ„äº‹é …
- éµå®ˆç¶²ç«™ä½¿ç”¨æ¢æ¬¾
- å°Šé‡ robots.txt
- é¿å…éåº¦è«‹æ±‚
- æ³¨æ„ç‰ˆæ¬Šå•é¡Œ
:::

### 5. çœ¾åŒ…æ¨™è¨»

ä½¿ç”¨å¹³å°å°‹æ±‚å¹«åŠ©ï¼š
- Amazon Mechanical Turk
- Figure Eight
- è‡ªå»ºæ¨™è¨»å¹³å°

## ğŸ“ è³‡æ–™æ”¶é›†æœ€ä½³å¯¦è¸

### 1. å®šç¾©æ˜ç¢ºçš„ç¯„åœ

```markdown
## å°ˆæ¡ˆç¯„åœå®šç¾©

**é ˜åŸŸï¼š** é›»å•†å®¢æœ
**èªè¨€ï¼š** ç¹é«”ä¸­æ–‡
**ä¸»é¡Œï¼š**
- ç”¢å“è«®è©¢
- è¨‚å–®æŸ¥è©¢
- é€€æ›è²¨æ”¿ç­–
- ä»˜æ¬¾å•é¡Œ
- é…é€è³‡è¨Š

**ä¸åŒ…å«ï¼š**
- æŠ€è¡“æ”¯æ´
- æ³•å¾‹è«®è©¢
- é†«ç™‚å»ºè­°
```

### 2. å»ºç«‹è³‡æ–™æ”¶é›†æª¢æŸ¥æ¸…å–®

```markdown
â–¡ è³‡æ–™ä¾†æºå·²ç¢ºèª
â–¡ æˆæ¬Šè¨±å¯å·²æª¢æŸ¥
â–¡ éš±ç§å•é¡Œå·²è™•ç†
â–¡ è³‡æ–™æ ¼å¼å·²çµ±ä¸€
â–¡ å“è³ªæ¨™æº–å·²å®šç¾©
â–¡ æ¨£æœ¬å·²æŠ½æŸ¥é©—è­‰
```

### 3. ç¶­è­·è³‡æ–™ç›®éŒ„

```python
# è³‡æ–™çµ„ç¹”çµæ§‹ç¯„ä¾‹
data/
â”œâ”€â”€ raw/                    # åŸå§‹è³‡æ–™
â”‚   â”œâ”€â”€ customer_logs.csv
â”‚   â”œâ”€â”€ faq_scraped.json
â”‚   â””â”€â”€ synthetic_data.jsonl
â”œâ”€â”€ processed/              # è™•ç†å¾Œè³‡æ–™
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ validation.jsonl
â”‚   â””â”€â”€ test.jsonl
â””â”€â”€ metadata/               # è³‡æ–™æè¿°
    â”œâ”€â”€ sources.md
    â”œâ”€â”€ statistics.json
    â””â”€â”€ changelog.md
```

## ğŸ”§ è³‡æ–™æ”¶é›†å·¥å…·

### Python è…³æœ¬ç¯„ä¾‹

```python
import json
import pandas as pd
from pathlib import Path

class DataCollector:
    def __init__(self, output_dir="data/raw"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.collected_data = []
    
    def add_from_csv(self, filepath, question_col, answer_col):
        """å¾ CSV æ–°å¢è³‡æ–™"""
        df = pd.read_csv(filepath)
        for _, row in df.iterrows():
            self.collected_data.append({
                'question': row[question_col],
                'answer': row[answer_col],
                'source': 'csv',
                'source_file': filepath
            })
    
    def add_from_json(self, filepath):
        """å¾ JSON æ–°å¢è³‡æ–™"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for item in data:
                self.collected_data.append({
                    **item,
                    'source': 'json',
                    'source_file': filepath
                })
    
    def add_manual(self, question, answer, metadata=None):
        """æ‰‹å‹•æ–°å¢è³‡æ–™"""
        self.collected_data.append({
            'question': question,
            'answer': answer,
            'source': 'manual',
            'metadata': metadata or {}
        })
    
    def save(self, filename="collected_data.jsonl"):
        """å„²å­˜æ”¶é›†çš„è³‡æ–™"""
        output_path = self.output_dir / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in self.collected_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"å·²å„²å­˜ {len(self.collected_data)} ç­†è³‡æ–™è‡³ {output_path}")
    
    def get_statistics(self):
        """å–å¾—è³‡æ–™çµ±è¨ˆ"""
        return {
            'total_count': len(self.collected_data),
            'sources': pd.Series([d['source'] for d in self.collected_data]).value_counts().to_dict(),
            'avg_question_length': sum(len(d['question']) for d in self.collected_data) / len(self.collected_data),
            'avg_answer_length': sum(len(d['answer']) for d in self.collected_data) / len(self.collected_data),
        }

# ä½¿ç”¨ç¯„ä¾‹
collector = DataCollector()
collector.add_from_csv('customer_logs.csv', 'question', 'answer')
collector.add_manual("ä½ å€‘çš„ç‡Ÿæ¥­æ™‚é–“ï¼Ÿ", "æˆ‘å€‘çš„ç‡Ÿæ¥­æ™‚é–“æ˜¯é€±ä¸€è‡³é€±äº” 9:00-18:00")
print(collector.get_statistics())
collector.save()
```

## âœ… è³‡æ–™æ”¶é›†æª¢æŸ¥è¡¨

### æ³•å¾‹èˆ‡å€«ç†

- [ ] ç¢ºèªè³‡æ–™ä½¿ç”¨æ¬Šé™
- [ ] éµå®ˆéš±ç§ä¿è­·æ³•è¦ï¼ˆGDPRã€å€‹è³‡æ³•ç­‰ï¼‰
- [ ] ç§»é™¤å€‹äººè­˜åˆ¥è³‡è¨Šï¼ˆPIIï¼‰
- [ ] å–å¾—å¿…è¦æˆæ¬Š

### å“è³ªæ§åˆ¶

- [ ] å®šç¾©å“è³ªæ¨™æº–
- [ ] åŸ·è¡Œæ¨£æœ¬æŠ½æŸ¥
- [ ] å»é™¤é‡è¤‡è³‡æ–™
- [ ] é©—è­‰è³‡æ–™å®Œæ•´æ€§

### æ–‡ä»¶è¨˜éŒ„

- [ ] è¨˜éŒ„è³‡æ–™ä¾†æº
- [ ] èªªæ˜æ”¶é›†æ–¹æ³•
- [ ] è¨»æ˜æ”¶é›†æ—¥æœŸ
- [ ] è¨˜éŒ„è³‡æ–™çµ±è¨ˆ

## ğŸ¯ å¸¸è¦‹æŒ‘æˆ°èˆ‡è§£æ±º

### æŒ‘æˆ° 1ï¼šè³‡æ–™é‡ä¸è¶³

**è§£æ±ºæ–¹æ¡ˆï¼š**
1. ä½¿ç”¨è³‡æ–™å¢å¼·æŠ€è¡“
2. åˆæˆè³‡æ–™ç”Ÿæˆ
3. ä¸»å‹•å­¸ç¿’ï¼ˆé¸æ“‡æ€§æ¨™è¨»ï¼‰
4. å°‘æ¨£æœ¬å­¸ç¿’æŠ€è¡“

### æŒ‘æˆ° 2ï¼šè³‡æ–™å“è³ªåƒå·®

**è§£æ±ºæ–¹æ¡ˆï¼š**
1. å»ºç«‹æ˜ç¢ºçš„å“è³ªæ¨™æº–
2. å¤šè¼ªäººå·¥å¯©æ ¸
3. è‡ªå‹•åŒ–å“è³ªæª¢æŸ¥
4. è¿­ä»£æ”¹é€²æµç¨‹

### æŒ‘æˆ° 3ï¼šè³‡æ–™ä¸å¹³è¡¡

**è§£æ±ºæ–¹æ¡ˆï¼š**
1. éæ¡æ¨£å°‘æ•¸é¡åˆ¥
2. æ¬ æ¡æ¨£å¤šæ•¸é¡åˆ¥
3. åˆæˆå°‘æ•¸é¡åˆ¥è³‡æ–™
4. èª¿æ•´æå¤±å‡½æ•¸æ¬Šé‡

## ä¸‹ä¸€æ­¥

è³‡æ–™æ”¶é›†å®Œæˆå¾Œï¼š

- ğŸ“ [è³‡æ–™æ¸…ç†èˆ‡è™•ç†](./data-cleaning)
- ğŸ·ï¸ [è³‡æ–™æ¨™è¨»](./data-annotation)
- ğŸ“‹ [è³‡æ–™é›†æ ¼å¼](./dataset-format)

## åƒè€ƒè³‡æº

- [Hugging Face Datasets](https://huggingface.co/datasets)
- [ä¸­æ–‡ NLP è³‡æº](https://github.com/crownpku/Awesome-Chinese-NLP)
- [è³‡æ–™å¢å¼·æŠ€è¡“](https://github.com/makcedward/nlpaug)
