---
sidebar_position: 4
---

# ç¬¬ä¸€å€‹å¾®èª¿å°ˆæ¡ˆ

é€™æ˜¯ä¸€å€‹å®Œæ•´çš„å¯¦æˆ°æ•™å­¸ï¼Œå°‡å¸¶ä½ å¾é›¶é–‹å§‹å®Œæˆç¬¬ä¸€å€‹ AI æ¨¡å‹å¾®èª¿å°ˆæ¡ˆã€‚æˆ‘å€‘å°‡ä½¿ç”¨ Google Colab å’Œ QLoRA æŠ€è¡“ï¼Œè¨“ç·´ä¸€å€‹ä¸­æ–‡å®¢æœæ©Ÿå™¨äººã€‚

## ğŸ¯ å°ˆæ¡ˆç›®æ¨™

è¨“ç·´ä¸€å€‹èƒ½å›ç­”ç‰¹å®šç”¢å“å•é¡Œçš„å®¢æœæ©Ÿå™¨äººï¼Œä¾‹å¦‚ï¼š

**è¼¸å…¥ï¼š** ä½ å€‘çš„ç”¢å“ä¿å›ºæœŸæ˜¯å¤šä¹…ï¼Ÿ
**è¼¸å‡ºï¼š** æˆ‘å€‘çš„ç”¢å“æä¾›ä¸€å¹´å…è²»ä¿å›ºæœå‹™ã€‚

## ğŸ“‹ æº–å‚™å·¥ä½œ

### 1. é–‹å•Ÿ Google Colab

1. å‰å¾€ [Google Colab](https://colab.research.google.com/)
2. å»ºç«‹æ–°ç­†è¨˜æœ¬
3. å•Ÿç”¨ GPUï¼š**åŸ·è¡Œéšæ®µ** â†’ **è®Šæ›´åŸ·è¡Œéšæ®µé¡å‹** â†’ é¸æ“‡ **T4 GPU**

### 2. å®‰è£å¿…è¦å¥—ä»¶

```python
# å®‰è£æ‰€éœ€å¥—ä»¶
!pip install -q transformers datasets accelerate peft bitsandbytes trl
```

## ğŸ”§ æ­¥é©Ÿ 1ï¼šè¼‰å…¥æ¨¡å‹å’Œ Tokenizer

```python
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer

# é¸æ“‡åŸºç¤æ¨¡å‹
model_name = "Qwen/Qwen-7B"

# é…ç½® 4-bit é‡åŒ–
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# è¼‰å…¥æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# è¼‰å…¥ tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token
```

## ğŸ“Š æ­¥é©Ÿ 2ï¼šæº–å‚™è¨“ç·´è³‡æ–™

```python
from datasets import Dataset

# ç¯„ä¾‹è³‡æ–™ï¼šå®¢æœå•ç­”å°
data = [
    {
        "instruction": "ä½ å€‘çš„ç”¢å“ä¿å›ºæœŸæ˜¯å¤šä¹…ï¼Ÿ",
        "output": "æˆ‘å€‘çš„ç”¢å“æä¾›ä¸€å¹´å…è²»ä¿å›ºæœå‹™ï¼Œåœ¨ä¿å›ºæœŸå…§å¦‚æœ‰ä»»ä½•å“è³ªå•é¡Œéƒ½å¯ä»¥å…è²»ç¶­ä¿®æˆ–æ›´æ›ã€‚"
    },
    {
        "instruction": "å¦‚ä½•è¯ç¹«å®¢æœï¼Ÿ",
        "output": "æ‚¨å¯ä»¥é€éä»¥ä¸‹æ–¹å¼è¯ç¹«æˆ‘å€‘ï¼š\n1. å®¢æœé›»è©±ï¼š0800-123-456ï¼ˆé€±ä¸€è‡³é€±äº” 9:00-18:00ï¼‰\n2. é›»å­éƒµä»¶ï¼šservice@example.com\n3. ç·šä¸Šå®¢æœï¼šå®˜ç¶²å³ä¸‹è§’çš„èŠå¤©æŒ‰éˆ•"
    },
    {
        "instruction": "å¯ä»¥é€€è²¨å—ï¼Ÿ",
        "output": "è³¼è²·å¾Œ7å¤©å…§ï¼Œå¦‚å•†å“æœªä½¿ç”¨ä¸”åŒ…è£å®Œæ•´ï¼Œå¯ç”³è«‹é€€è²¨ã€‚è«‹è¯ç¹«å®¢æœå–å¾—é€€è²¨æˆæ¬Šç¢¼ã€‚"
    },
    {
        "instruction": "é‹é€éœ€è¦å¤šä¹…æ™‚é–“ï¼Ÿ",
        "output": "è¨‚å–®ç¢ºèªå¾Œï¼Œä¸€èˆ¬åœ¨3-5å€‹å·¥ä½œå¤©å…§é€é”ã€‚åé åœ°å€å¯èƒ½éœ€è¦é¡å¤–1-2å¤©ã€‚"
    },
    {
        "instruction": "ä»˜æ¬¾æ–¹å¼æœ‰å“ªäº›ï¼Ÿ",
        "output": "æˆ‘å€‘æ¥å—ä»¥ä¸‹ä»˜æ¬¾æ–¹å¼ï¼š\n1. ä¿¡ç”¨å¡ï¼ˆVisaã€MasterCardã€JCBï¼‰\n2. ATMè½‰å¸³\n3. è²¨åˆ°ä»˜æ¬¾ï¼ˆé™å°ç£æœ¬å³¶ï¼‰\n4. è¶…å•†ä»£æ”¶"
    },
]

# è½‰æ›ç‚º Dataset æ ¼å¼
dataset = Dataset.from_list(data)

# æ ¼å¼åŒ–è³‡æ–™
def format_instruction(sample):
    return f"""### æŒ‡ä»¤ï¼š
{sample['instruction']}

### å›æ‡‰ï¼š
{sample['output']}"""

# æ‡‰ç”¨æ ¼å¼åŒ–
dataset = dataset.map(lambda x: {"text": format_instruction(x)})
```

:::tip å¯¦éš›æ‡‰ç”¨
åœ¨å¯¦éš›å°ˆæ¡ˆä¸­ï¼Œä½ éœ€è¦æº–å‚™ 100-1000 ç­†ä»¥ä¸Šçš„é«˜å“è³ªå•ç­”å°ã€‚è³‡æ–™è¶Šå¤šè¶Šå¥½ï¼Œä½†å“è³ªæ¯”æ•¸é‡æ›´é‡è¦ï¼
:::

## âš™ï¸ æ­¥é©Ÿ 3ï¼šé…ç½® LoRA

```python
# LoRA é…ç½®
lora_config = LoraConfig(
    r=16,                      # LoRA rank
    lora_alpha=32,             # LoRA alpha
    target_modules=[           # è¦è¨“ç·´çš„æ¨¡çµ„
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
    ],
    lora_dropout=0.05,         # Dropout
    bias="none",
    task_type="CAUSAL_LM"
)

# æ‡‰ç”¨ LoRA åˆ°æ¨¡å‹
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

## ğŸƒ æ­¥é©Ÿ 4ï¼šè¨“ç·´æ¨¡å‹

```python
# è¨“ç·´åƒæ•¸
training_args = TrainingArguments(
    output_dir="./results",              # è¼¸å‡ºç›®éŒ„
    num_train_epochs=3,                  # è¨“ç·´è¼ªæ•¸
    per_device_train_batch_size=1,       # æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=4,       # æ¢¯åº¦ç´¯ç©
    learning_rate=2e-4,                  # å­¸ç¿’ç‡
    logging_steps=10,                    # æ—¥èªŒé »ç‡
    save_steps=50,                       # å„²å­˜é »ç‡
    save_total_limit=2,                  # æœ€å¤šä¿ç•™æª¢æŸ¥é»æ•¸
    fp16=True,                           # æ··åˆç²¾åº¦è¨“ç·´
    report_to="none",                    # ä¸ä½¿ç”¨å¤–éƒ¨æ—¥èªŒå·¥å…·
)

# å»ºç«‹è¨“ç·´å™¨
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=training_args,
    peft_config=lora_config,
    dataset_text_field="text",
    tokenizer=tokenizer,
    max_seq_length=512,
)

# é–‹å§‹è¨“ç·´
trainer.train()
```

## ğŸ’¾ æ­¥é©Ÿ 5ï¼šå„²å­˜æ¨¡å‹

```python
# å„²å­˜ LoRA æ¬Šé‡
trainer.model.save_pretrained("./fine-tuned-model")
tokenizer.save_pretrained("./fine-tuned-model")

print("æ¨¡å‹å·²å„²å­˜è‡³ ./fine-tuned-model")
```

## ğŸ§ª æ­¥é©Ÿ 6ï¼šæ¸¬è©¦æ¨¡å‹

```python
# è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹é€²è¡Œæ¸¬è©¦
from peft import PeftModel

# é‡æ–°è¼‰å…¥åŸºç¤æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# è¼‰å…¥ LoRA æ¬Šé‡
model = PeftModel.from_pretrained(base_model, "./fine-tuned-model")

# æ¸¬è©¦å‡½æ•¸
def generate_response(instruction):
    prompt = f"""### æŒ‡ä»¤ï¼š
{instruction}

### å›æ‡‰ï¼š
"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # åªè¿”å›å›æ‡‰éƒ¨åˆ†
    return response.split("### å›æ‡‰ï¼š")[-1].strip()

# æ¸¬è©¦ç¯„ä¾‹
test_questions = [
    "ç”¢å“ä¿å›ºå¤šä¹…ï¼Ÿ",
    "å¦‚ä½•é€€è²¨ï¼Ÿ",
    "å®¢æœé›»è©±æ˜¯ä»€éº¼ï¼Ÿ"
]

for question in test_questions:
    print(f"å•é¡Œï¼š{question}")
    print(f"å›ç­”ï¼š{generate_response(question)}")
    print("-" * 50)
```

## ğŸ“ˆ ç›£æ§è¨“ç·´é€²åº¦

è¨“ç·´éç¨‹ä¸­ä½ æœƒçœ‹åˆ°é¡ä¼¼çš„è¼¸å‡ºï¼š

```
Step 10: loss=2.345
Step 20: loss=1.987
Step 30: loss=1.654
...
```

**Loss ä¸‹é™** = æ¨¡å‹åœ¨å­¸ç¿’ï¼

ç†æƒ³æƒ…æ³ï¼š
- Loss æŒçºŒä¸‹é™
- æœ€çµ‚æ”¶æ–‚åˆ°è¼ƒä½å€¼ï¼ˆ< 1.0ï¼‰

## ğŸ“ ç†è§£è¼¸å‡º

### è¨“ç·´åƒæ•¸èªªæ˜

```
trainable params: 8,388,608 || all params: 7,241,728,000 || trainable%: 0.1158
```

é€™è¡¨ç¤ºï¼š
- åªæœ‰ 0.12% çš„åƒæ•¸è¢«è¨“ç·´ï¼ˆLoRA çš„å¨åŠ›ï¼ï¼‰
- å¤§å¹…æ¸›å°‘è¨˜æ†¶é«”å’Œé‹ç®—éœ€æ±‚

### è¨“ç·´æ™‚é–“ä¼°è¨ˆ

åœ¨ Colab T4 GPU ä¸Šï¼š
- 5 ç­†è³‡æ–™ï¼šç´„ 5-10 åˆ†é˜
- 100 ç­†è³‡æ–™ï¼šç´„ 1-2 å°æ™‚
- 1000 ç­†è³‡æ–™ï¼šç´„ 10-20 å°æ™‚

## ğŸ”§ å¸¸è¦‹å•é¡Œèˆ‡è§£æ±º

### è¨˜æ†¶é«”ä¸è¶³ï¼ˆOOMï¼‰

```python
# æ¸›å°‘æ‰¹æ¬¡å¤§å°
per_device_train_batch_size=1

# å¢åŠ æ¢¯åº¦ç´¯ç©
gradient_accumulation_steps=8

# æ¸›å°‘åºåˆ—é•·åº¦
max_seq_length=256
```

### è¨“ç·´ä¸ç©©å®š

```python
# é™ä½å­¸ç¿’ç‡
learning_rate=1e-4

# å¢åŠ  warmup
warmup_steps=100
```

### æ•ˆæœä¸ç†æƒ³

1. **å¢åŠ è¨“ç·´è³‡æ–™**ï¼šæ›´å¤šé«˜å“è³ªè³‡æ–™
2. **å¢åŠ è¨“ç·´è¼ªæ•¸**ï¼šå¾ 3 å¢åŠ åˆ° 5-10
3. **èª¿æ•´ LoRA rank**ï¼šå¢åŠ  r å€¼ï¼ˆå¦‚ 32ï¼‰

## ğŸ“¦ ä¸‹è¼‰æ¨¡å‹åˆ°æœ¬åœ°

```python
from google.colab import files

# å£“ç¸®æ¨¡å‹æª”æ¡ˆ
!zip -r fine-tuned-model.zip ./fine-tuned-model

# ä¸‹è¼‰
files.download('fine-tuned-model.zip')
```

## âœ… å®Œæ•´ç¨‹å¼ç¢¼ç¯„æœ¬

å®Œæ•´çš„å¯åŸ·è¡Œ Colab Notebookï¼š

ğŸ‘‰ [é»æ­¤é–‹å•Ÿ Colab Notebook](https://colab.research.google.com/)

## ğŸ¯ ä¸‹ä¸€æ­¥

æ­å–œå®Œæˆç¬¬ä¸€å€‹å¾®èª¿å°ˆæ¡ˆï¼æ¥ä¸‹ä¾†ä½ å¯ä»¥ï¼š

1. **æº–å‚™æ›´å¤šè³‡æ–™**ï¼š[è³‡æ–™æº–å‚™æŒ‡å—](../data-preparation/data-collection)
2. **æ·±å…¥å­¸ç¿’ LoRA**ï¼š[LoRA é€²éšæŠ€å·§](../fine-tuning/lora)
3. **éƒ¨ç½²æ¨¡å‹**ï¼š[æ¨¡å‹éƒ¨ç½²](../deployment/local)
4. **å„ªåŒ–æ•ˆèƒ½**ï¼š[æ•ˆèƒ½å„ªåŒ–](../evaluation/optimization)

## ğŸ’¡ å¯¦æˆ°æŠ€å·§

### è³‡æ–™å“è³ªæœ€é‡è¦
- å¯§å°‘å‹¿æ¿«ï¼š10 ç­†é«˜å“è³ª > 100 ç­†ä½å“è³ª
- æ¶µè“‹å¤šæ¨£æ€§ï¼šç¢ºä¿è³‡æ–™æ¶µè“‹å„ç¨®æƒ…æ³
- æ ¼å¼ä¸€è‡´ï¼šçµ±ä¸€å•ç­”æ ¼å¼

### è¿­ä»£å„ªåŒ–
1. å¾å°è¦æ¨¡é–‹å§‹ï¼ˆ10-20 ç­†ï¼‰
2. å¿«é€Ÿè¨“ç·´å’Œæ¸¬è©¦
3. ç™¼ç¾å•é¡Œä¸¦æ”¹é€²è³‡æ–™
4. é€æ­¥æ“´å¤§è¦æ¨¡

### ç‰ˆæœ¬ç®¡ç†
- è¨˜éŒ„æ¯æ¬¡å¯¦é©—çš„åƒæ•¸
- ä¿å­˜ä¸åŒç‰ˆæœ¬çš„æ¨¡å‹
- æ¯”è¼ƒä¸åŒç‰ˆæœ¬çš„æ•ˆæœ

## åƒè€ƒè³‡æº

- [Hugging Face Transformers æ–‡ä»¶](https://huggingface.co/docs/transformers)
- [PEFT æ–‡ä»¶](https://huggingface.co/docs/peft)
- [TRL æ–‡ä»¶](https://huggingface.co/docs/trl)
