---
sidebar_position: 3
---

# é¸æ“‡åˆé©çš„æ¨¡å‹

é¸æ“‡æ­£ç¢ºçš„åŸºç¤æ¨¡å‹æ˜¯æˆåŠŸè¨“ç·´çš„ç¬¬ä¸€æ­¥ã€‚æœ¬æŒ‡å—å°‡å¹«åŠ©ä½ æ ¹æ“šéœ€æ±‚å’Œè³‡æºé¸æ“‡æœ€é©åˆçš„æ¨¡å‹ã€‚

## ğŸ¯ é¸æ“‡è€ƒé‡å› ç´ 

### 1. ä»»å‹™é¡å‹

ä¸åŒçš„ä»»å‹™éœ€è¦ä¸åŒé¡å‹çš„æ¨¡å‹ï¼š

#### æ–‡å­—ç”Ÿæˆï¼ˆText Generationï¼‰
- **ç”¨é€”**ï¼šèŠå¤©æ©Ÿå™¨äººã€æ–‡ç« æ’°å¯«ã€ç¨‹å¼ç¢¼ç”Ÿæˆ
- **æ¨è–¦æ¨¡å‹**ï¼šGPT ç³»åˆ—ã€LLaMAã€Mistral

#### æ–‡å­—ç†è§£ï¼ˆText Understandingï¼‰
- **ç”¨é€”**ï¼šæ–‡æœ¬åˆ†é¡ã€æƒ…æ„Ÿåˆ†æã€å‘½åå¯¦é«”è­˜åˆ¥
- **æ¨è–¦æ¨¡å‹**ï¼šBERTã€RoBERTa

#### å•ç­”ç³»çµ±ï¼ˆQuestion Answeringï¼‰
- **ç”¨é€”**ï¼šå®¢æœæ©Ÿå™¨äººã€çŸ¥è­˜å•ç­”
- **æ¨è–¦æ¨¡å‹**ï¼šGPT ç³»åˆ—ã€T5

#### ç¿»è­¯ï¼ˆTranslationï¼‰
- **ç”¨é€”**ï¼šå¤šèªè¨€ç¿»è­¯
- **æ¨è–¦æ¨¡å‹**ï¼šMarianMTã€T5

### 2. èªè¨€æ”¯æ´

#### å¤šèªè¨€æ¨¡å‹
- **XLM-RoBERTa**ï¼šæ”¯æ´ 100+ èªè¨€
- **mBERT**ï¼šå¤šèªè¨€ BERT
- **BLOOM**ï¼šæ”¯æ´ 46 ç¨®èªè¨€

#### ä¸­æ–‡å°ˆç”¨æ¨¡å‹
- **BERT-base-chinese**ï¼šä¸­æ–‡ BERT
- **Chinese-LLaMA**ï¼šä¸­æ–‡ LLaMA
- **ChatGLM**ï¼šæ¸…è¯å¤§å­¸é–‹æºå°è©±æ¨¡å‹
- **Baichuan**ï¼šç™¾å·æ™ºèƒ½é–‹æºæ¨¡å‹
- **Qwen**ï¼šé˜¿é‡Œé›²é€šç¾©åƒå•

### 3. æ¨¡å‹è¦æ¨¡èˆ‡è³‡æºéœ€æ±‚

| æ¨¡å‹é¡å‹ | åƒæ•¸é‡ | GPU è¨˜æ†¶é«”éœ€æ±‚ | é©ç”¨å¹³å° |
|---------|--------|----------------|----------|
| æ¥µå°å‹ | < 500M | 2-4GB | Colab å…è²»ç‰ˆ |
| å°å‹ | 500M-1B | 4-8GB | Colab å…è²»ç‰ˆ |
| ä¸­å‹ | 1B-7B | 8-16GB | Colab Pro / RTX 3060 |
| å¤§å‹ | 7B-13B | 16-24GB | RTX 3090 / 4090 |
| è¶…å¤§å‹ | 13B+ | 24GB+ | A100 / å¤š GPU |

:::tip è¨˜æ†¶é«”è¨ˆç®—
æ¨¡å‹è¨˜æ†¶é«”éœ€æ±‚ï¼ˆFP16ï¼‰â‰ˆ åƒæ•¸é‡ Ã— 2 bytes
ä¾‹å¦‚ï¼š7B æ¨¡å‹ â‰ˆ 14GBï¼ˆåƒ…è¼‰å…¥æ¨¡å‹ï¼Œä¸å«è¨“ç·´ï¼‰
:::

## ğŸ¤– æ¨è–¦æ¨¡å‹åˆ—è¡¨

### å…¥é–€æ¨è–¦ï¼ˆé©åˆ Colab å…è²»ç‰ˆï¼‰

#### 1. BERT-base-chinese
```python
model_name = "bert-base-chinese"
```
- **åƒæ•¸é‡**ï¼š110M
- **è¨˜æ†¶é«”éœ€æ±‚**ï¼šç´„ 2GB
- **é©ç”¨ä»»å‹™**ï¼šæ–‡æœ¬åˆ†é¡ã€å‘½åå¯¦é«”è­˜åˆ¥ã€å•ç­”
- **å„ªé»**ï¼šè¼•é‡ã€å¿«é€Ÿã€ä¸­æ–‡æ”¯æ´å¥½
- **ç¼ºé»**ï¼šä¸é©åˆé•·æ–‡æœ¬ç”Ÿæˆ

#### 2. GPT2-chinese
```python
model_name = "uer/gpt2-chinese-cluecorpussmall"
```
- **åƒæ•¸é‡**ï¼š117M
- **è¨˜æ†¶é«”éœ€æ±‚**ï¼šç´„ 2GB
- **é©ç”¨ä»»å‹™**ï¼šæ–‡æœ¬ç”Ÿæˆã€å°è©±
- **å„ªé»**ï¼šè¼•é‡ã€é©åˆåˆå­¸è€…
- **ç¼ºé»**ï¼šèƒ½åŠ›ç›¸å°æœ‰é™

### ä¸­éšæ¨è–¦ï¼ˆé©åˆ Colab Pro / RTX 3060ï¼‰

#### 1. Chinese-LLaMA-2-7B
```python
model_name = "hfl/chinese-llama-2-7b"
```
- **åƒæ•¸é‡**ï¼š7B
- **è¨˜æ†¶é«”éœ€æ±‚**ï¼šç´„ 14GBï¼ˆå¯ç”¨ QLoRA é™è‡³ 6GBï¼‰
- **é©ç”¨ä»»å‹™**ï¼šé€šç”¨å°è©±ã€æŒ‡ä»¤è·Ÿéš¨
- **å„ªé»**ï¼šä¸­æ–‡èƒ½åŠ›å¼·ã€ç¤¾ç¾¤æ”¯æ´å¥½
- **ç¼ºé»**ï¼šéœ€è¦è¼ƒå¤§è¨˜æ†¶é«”

#### 2. ChatGLM3-6B
```python
model_name = "THUDM/chatglm3-6b"
```
- **åƒæ•¸é‡**ï¼š6B
- **è¨˜æ†¶é«”éœ€æ±‚**ï¼šç´„ 12GBï¼ˆå¯ç”¨ QLoRA é™è‡³ 5GBï¼‰
- **é©ç”¨ä»»å‹™**ï¼šå°è©±ã€å¤šè¼ªå°è©±
- **å„ªé»**ï¼šä¸­æ–‡åŸç”Ÿã€æ•ˆèƒ½å¥½
- **ç¼ºé»**ï¼šç‰¹æ®Šæ¶æ§‹ï¼Œéœ€é¡å¤–é©é…

#### 3. Qwen-7B
```python
model_name = "Qwen/Qwen-7B"
```
- **åƒæ•¸é‡**ï¼š7B
- **è¨˜æ†¶é«”éœ€æ±‚**ï¼šç´„ 14GBï¼ˆå¯ç”¨ QLoRA é™è‡³ 6GBï¼‰
- **é©ç”¨ä»»å‹™**ï¼šé€šç”¨å°è©±ã€ç¨‹å¼ç¢¼ç”Ÿæˆ
- **å„ªé»**ï¼šå¤šèªè¨€æ”¯æ´ã€æ•ˆèƒ½å„ªç§€
- **ç¼ºé»**ï¼šæ¨¡å‹è¼ƒæ–°ï¼Œæ–‡ä»¶è¼ƒå°‘

### é€²éšæ¨è–¦ï¼ˆé©åˆ RTX 3090 / 4090ï¼‰

#### 1. LLaMA-2-13B
```python
model_name = "meta-llama/Llama-2-13b-hf"
```
- **åƒæ•¸é‡**ï¼š13B
- **è¨˜æ†¶é«”éœ€æ±‚**ï¼šç´„ 26GBï¼ˆå¯ç”¨ QLoRA é™è‡³ 10GBï¼‰
- **é©ç”¨ä»»å‹™**ï¼šè¤‡é›œå°è©±ã€æ¨ç†
- **å„ªé»**ï¼šæ•ˆèƒ½å¼·å¤§ã€é–‹æºå‹å–„
- **ç¼ºé»**ï¼šä¸­æ–‡èƒ½åŠ›éœ€å¼·åŒ–

#### 2. Baichuan2-13B
```python
model_name = "baichuan-inc/Baichuan2-13B-Base"
```
- **åƒæ•¸é‡**ï¼š13B
- **è¨˜æ†¶é«”éœ€æ±‚**ï¼šç´„ 26GBï¼ˆå¯ç”¨ QLoRA é™è‡³ 10GBï¼‰
- **é©ç”¨ä»»å‹™**ï¼šé€šç”¨å°è©±
- **å„ªé»**ï¼šä¸­æ–‡èƒ½åŠ›å¼·
- **ç¼ºé»**ï¼šéœ€è¦è¼ƒå¤§è³‡æº

## ğŸ” æ¨¡å‹ä¾†æº

### Hugging Face Model Hub
æœ€å¤§çš„é–‹æºæ¨¡å‹å¹³å°ï¼Œæä¾›æ•¸è¬å€‹é è¨“ç·´æ¨¡å‹ã€‚

**ä½¿ç”¨æ–¹å¼ï¼š**
```python
from transformers import AutoModel, AutoTokenizer

model_name = "bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
```

**ç€è¦½æ¨¡å‹ï¼š** https://huggingface.co/models

### ModelScopeï¼ˆé­”æ­ï¼‰
é˜¿é‡Œé›²çš„æ¨¡å‹å¹³å°ï¼Œä¸­åœ‹åœ°å€è¨ªå•æ›´å¿«ã€‚

**ä½¿ç”¨æ–¹å¼ï¼š**
```python
from modelscope import AutoModel, AutoTokenizer

model_name = "damo/nlp_structbert_backbone_base_std"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
```

**ç€è¦½æ¨¡å‹ï¼š** https://modelscope.cn/models

## ğŸ¯ æ±ºç­–æµç¨‹åœ–

```
é–‹å§‹
  â†“
ä½ çš„ GPU è¨˜æ†¶é«”æ˜¯ï¼Ÿ
  â”œâ”€ < 8GB
  â”‚   â””â”€ ä½¿ç”¨ QLoRA + å°å‹æ¨¡å‹ï¼ˆ1-3Bï¼‰
  â”‚       æ¨è–¦ï¼šBERT-chinese, GPT2-chinese
  â”‚
  â”œâ”€ 8-16GB
  â”‚   â””â”€ ä½¿ç”¨ QLoRA + ä¸­å‹æ¨¡å‹ï¼ˆ3-7Bï¼‰
  â”‚       æ¨è–¦ï¼šChatGLM3-6B, LLaMA-2-7B
  â”‚
  â””â”€ > 16GB
      â””â”€ å¯é¸æ“‡å¤§å‹æ¨¡å‹ï¼ˆ7-13Bï¼‰
          æ¨è–¦ï¼šLLaMA-2-13B, Baichuan2-13B
```

## ğŸ“Š æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ

### ä¸­æ–‡èƒ½åŠ›

| æ¨¡å‹ | åƒæ•¸é‡ | ä¸­æ–‡æµæš¢åº¦ | æŒ‡ä»¤ç†è§£ | æ¨è–¦åº¦ |
|------|--------|-----------|---------|--------|
| ChatGLM3-6B | 6B | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| Qwen-7B | 7B | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| Chinese-LLaMA-2-7B | 7B | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| Baichuan2-13B | 13B | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| BERT-chinese | 110M | â­â­â­ | N/A | â­â­â­ |

### è³‡æºéœ€æ±‚

| æ¨¡å‹ | æœ€ä½ GPU | æ¨è–¦ GPU | è¨“ç·´é€Ÿåº¦ |
|------|---------|---------|---------|
| BERT-chinese | 4GB | 8GB | å¿« |
| GPT2-chinese | 4GB | 8GB | å¿« |
| ChatGLM3-6B | 8GB | 16GB | ä¸­ç­‰ |
| Qwen-7B | 8GB | 16GB | ä¸­ç­‰ |
| LLaMA-2-7B | 10GB | 16GB | ä¸­ç­‰ |
| Baichuan2-13B | 12GB | 24GB | æ…¢ |

## ğŸš€ å¿«é€Ÿè¼‰å…¥æ¨¡å‹

### åŸºæœ¬è¼‰å…¥

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen-7B"

# è¼‰å…¥ tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

# è¼‰å…¥æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    trust_remote_code=True
)
```

### ä½¿ç”¨é‡åŒ–è¼‰å…¥

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# 4-bit é‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
```

## âš ï¸ æ³¨æ„äº‹é …

### æˆæ¬Šè¨±å¯

æŸäº›æ¨¡å‹æœ‰ä½¿ç”¨é™åˆ¶ï¼Œè«‹å‹™å¿…æª¢æŸ¥ï¼š
- å•†æ¥­ä½¿ç”¨æ˜¯å¦å…è¨±
- æ˜¯å¦éœ€è¦ç”³è«‹æˆæ¬Š
- è³‡æ–™ä½¿ç”¨é™åˆ¶

### æ¨¡å‹å¿«å–

æ¨¡å‹ä¸‹è¼‰å¾Œæœƒå¿«å–åœ¨æœ¬åœ°ï¼š
- é è¨­ä½ç½®ï¼š`~/.cache/huggingface/`
- å¯è¨­å®šç’°å¢ƒè®Šæ•¸ï¼š`export HF_HOME=/path/to/cache`
- å¤§å‹æ¨¡å‹å¯èƒ½ä½”ç”¨ 10-30GB ç©ºé–“

### ç¶²è·¯å•é¡Œ

ä¸­åœ‹åœ°å€å¯èƒ½éœ€è¦ï¼š
- ä½¿ç”¨é¡åƒç«™ï¼ˆModelScopeï¼‰
- è¨­å®šä»£ç†
- æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ

## ä¸‹ä¸€æ­¥

é¸æ“‡å¥½æ¨¡å‹å¾Œï¼Œæ¥ä¸‹ä¾†ï¼š

- ğŸš€ [é–‹å§‹ç¬¬ä¸€å€‹å¾®èª¿å°ˆæ¡ˆ](./first-project)
- ğŸ“Š [æº–å‚™è¨“ç·´è³‡æ–™](../data-preparation/data-collection)
- ğŸ”§ [äº†è§£ LoRA å¾®èª¿](../fine-tuning/lora)

## åƒè€ƒè³‡æº

- [Hugging Face Model Hub](https://huggingface.co/models)
- [ModelScope](https://modelscope.cn/models)
- [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
